\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Code-Mixed Language Translation}



\author{
 \textbf{Adem Odza},
 \textbf{Abhirup Mukherjee},
 \textbf{Buvana Seshathri},
 \textbf{Samiha Kuncham},
\\
\\
 The University of Texas at Dallas,
 CS 6320 Natural Language Processing,
 Dr.\ Xinya Du
\\
 \small{
   \textbf{GitHub:} \href{https://github.com/AdemOdza/Multi-Lingual-Text-Translation}{https://github.com/AdemOdza/Multi-Lingual-Text-Translation}
 }
}

\begin{document}
\maketitle
\begin{abstract}
In our paper, we investigate and compare the performance of different architectures for the translation of code-mixed language. 
Code-mixed language translation requires the identification of languages as well as the tokenization of different scripts, which makes it a much more complex problem than typical translation tasks. 
In order to determine the best type of architecture for this problem, we evaluated different combinations of models and word embeddings by comparing their BLEU scores. 
We found that Transformers using the mBERT embeddings outperformed the other architectures. 
While the pre-trained multilingual embeddings performed well, the results suggest that architectures specifically designed and trained for code-mixed language translations would have a better performance. 
These findings should provide guidance for those looking to perform translation on texts with mixed languages. 
\end{abstract}

\section{Introduction}

Translation has always been a prominent task within natural language processing, especially with the rise of neural networks and machine learning. 
While existing translation tools are powerful, the rise of informal and code-mixed language poses a problem for these tools. 

Code-mixing is the act of combining words from two different languages in speech, typically done by bilingual speakers or between family members who have varying levels of fluency in their common language. 
The rise of social media and increasingly informal speech online has led to code-mixing becoming a common occurrence in online discussion. 
Social media platforms typically have translation features for their site content, but these do not always perform correctly when messages contain multi-lingual content. 
The mixing of languages leads to many problems with translation, such as tokenization of different scripts, precision/size of the tokens, handling languages with different morphemes, and preserving context between languages.

There are many differences between languages that most translation systems might not account for. 
Languages like English and Spanish might be similarly structured enough to where there is not a significant difference in performance, but what about more distant languages like English and Hindi? 
They use the Latin and Devanagari scripts respectively, English uses Subject-Verb-Object order while Hindi uses Subject-Object-Verb, and they have varying amounts of Vowels and Consonants. 
This poses a significant challenge for tokenization and representation of the words in a common way. 
There is also the problem of identifying which words are in which language and preserving context between languages.

In our paper, we investigate some of the existing models available and compare the results of different architectures that incorporate these models. 
We investigated architectures using different word embeddings along with different translation models. 
These architectures were trained and tested with a mixed English-Hindi language corpus as well as some additional Zero-Shot testing with a mixed English-Spanish corpus. 
We then evaluated the models based on their BLEU scores. 


\section{Related Work}
Work in machine translation and multilingual natural language processing has improved a lot over the recent years, especially in handling the unique cases that are introduced by code\-mixed languages.

Modern neural machine translation (NMT) has given us the foundation of the sequence\-to\-sequence (seq2seq) paradigm [5], which learns an end\-to\-end mapping from an input sequence to a target sequence using neural networks. Sutskever et al. introduced the canonical seq2seq model that uses multilayer Long Short\-Term Memory (LSTM) encoders and decoders to transform variable\-length inputs into fixed\-length representations, and then decode these representations into target sequences. This paper showed good performance on English–French translation benchmarks compared to traditional statistical methods. 
These seq2seq models were used as the foundation for neural approaches for translation and other sequence prediction tasks. This was done by showing that recurrent architectures could learn representations for both encoding and decoding language data without human intervention.

Even though we see these advances in neural translation, the code\-mixed language translation part has many other challenges due to the frequent blending of languages while people speak and the lack of annotated parallel data (to verify against). To address such data limitations, recent work has focused on synthetic data generation. The HINMIX\_hi\-en dataset that we used in this project gave us large\-scale parallel Hinglish\-English data by using bilingual corpus data and synthetic code\-mixing techniques.

Using this work, Kartik et al.\ provided a framework that uses synthetic data generation with joint learning that improves translation quality for noisy, code-mixed text. Their method generated large code-mixed data [1]. It also uses joint training to share parameters around clean and noisy examples, which means that it gave an improved performance even in zero-shot transfer to new code-mixed variants.

Another major trend in NLP has been the pre\-training and fine\-tuning process used by models like BERT (Bidirectional Encoder Representations from Transformers). BERT gave us a deep bidirectional contextual embedding method that captured language representations from large unlabeled data. BERT is also used in many downstream tasks that includes classification, inference, and translation related tasks after doing fine\-tuning.

Even though BERT itself is not a sequence\-to\-sequence model, the idea of using a pretrained contextual encoder has affected later translation architectures. It has mainly affected transformer\-based models that use pretrained representations or attention mechanisms. These pretrained models improved generalization for code\-mixed translation where annotated resources are extremely limited.

Previous work has shown that mBERT can be used for code\-mixed and low\-resource translation settings. This is done by making use of mBERT's ability to model mixed\-language context and cross\-lingual semantics.\ mBERT (Multilingual BERT) is an extension of BERT to 100+ languages given to us by jointly pretraining on large multilingual data.


\section{Data}
The dataset that was used in this project is the HINMIX dataset. HINMIX is a massive parallel code-mixed dataset for Hindi-English code switching. 

\begin{itemize}
    \item This dataset contains $4.2$M fully parallel sentences in 6 Hindi-English forms.
    \item It also consists of a validation set with $280$ examples and a test set with $2507$ samples. 
    \item HINMIX is a synthetic dataset. 
    \item Hicm includes Hindi sentences with codemix words substituted in English. 
    \item It focuses on Hinglish sentences where Hindi and English are blended in a natural, conversational way.
    \item This dataset consists of $391$,$000$ unique tokens with both Hindi and English.
    \item Each Hinglish sentence is paired with a human-annotated English translation.
\end{itemize}

\section{Methodology}
We describe the preprocessing methods, model implementations, and model evaluation in this section.

\subsection{Data Preprocessing}
We had the following approach for tokenizing the data - mBERT tokenizer. The BERT tokenizer uses BPE (byte-pair encoding) subword tokenization. We used the BertTokenizer from HuggingFace transformers [4]. OOV words are handled using subwords. This particular tokenizer had been pretrained on $104$ languages. We limited our max sequence length to $128$ - anything longer was truncated. [3]

\subsection{Model Architectures}
We used a simple transformer architecture with self-attention mechanism. This way, we are able to process sequences in parallel and capture long-range dependencies - often surpassing RNNs.

Our transformer architecture (hidden dims = $256$) was designed as follows:
\begin{enumerate}
    \item The encoder consisted of three layers - mutli-head self attention (four heads), FFNN (with a dimension of 1024), layer normalization
    \item The decoder had 3 layers that incorporated masked self-attention and cross-attention
    \item Sinusoidal positional embedding
\end{enumerate}
A dropout rate (0.01) was applied after the attention and feed-forward layers.

Our seq2seq architecture was designed as follows:
\begin{enumerate}
    \item Bidirectional LSTM encoder two layers and $256$ dimensions (since the bidirectional LSTM concatenates forward and backward hidden states)
    \item LSTM decoder
    \item BahadanauAttention [2] - This is a differentiable attention model without the unidirectional alignment limitation. When predicting a token, if not all the input tokens are relevant, the model aligns (or attends) only to parts of the input sequence that are deemed relevant to the current prediction. This is then used to update the current state before generating the next token.
\end{enumerate}

\begin{table}[]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Hyperparameter}} & \multicolumn{1}{c|}{\textbf{Transformer}} & \textbf{Seq2Seq} \\ \hline
Hidden Dimension                              & $256$                                       & $256$              \\ \hline
Layers                                        & $2$                                         & $2$                \\ \hline
Attention Heads                               & $8$                                         & -                \\ \hline
Feed-Forward Dim.                             & $1024$                                      & -                \\ \hline
Dropout                                       & $0.1$                                       & $0.1$              \\ \hline
Embedding Dim.                                & $768$ to $256$                                & $768$ to $256$       \\ \hline
\end{tabular}%
}
\caption{Hyperparameters on the Transformer and Seq2Seq models}\label{tab:hyperparams}
\end{table}

\subsection{Embeddings}
We used the frozen bert-base-multilingual-cased embeddings - this is the pretrained BERT embedding trained on a massive multilingual corpus. We did this to preserve the mutli-lingual embedding space and also reduce our trainable parameters for our models. We also hoped that using pre-trained embeddings would give the model better zero-shot capabilities.

\subsection{Evaluation}
For evaluation, we used the BLEU (Bilingual Evaluation Understudy) for our primary evaluation metric - we used the sacrebleu library. The BLEU score is a measure of n-gram overlap between the predicted and reference translations. The scores range between $0$-$100$ with models around the $20$-$30$ BLEU points producing coherent translations.

\section{Experiments and Results}

\subsection{Architecture and Embedding Comparison}

We evaluated Transformer and Seq2Seq architectures using both pre-trained multilingual embeddings (mBERT) and learned embeddings. The results can be viewed in Table~\ref{tab:archresults}. 
\begin{table}[]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Architecture}} & \multicolumn{1}{c|}{\textbf{Epochs}} & \multicolumn{1}{c|}{\textbf{Training Time (Hrs.)}} & \multicolumn{1}{c|}{\textbf{BLEU Score}} \\ \hline
Transformer + mBERT                         & $10$                                   & $23$                                                 & \textbf{$13.70$}                           \\ \hline
Seq2Seq + mBERT                             & $3$                                    & $1$                                                  & $0.09$                                     \\ \hline
Transformer + Learned Embeddings            & $10$                                   & $1.25$                                               & $1.26$                                     \\ \hline
Seq2Seq + Learned Embeddings                & $2$                                    & $5$                                                  & $1.83$                                     \\ \hline
\end{tabular}%
}
\caption{The Architectures and their associated BLEU scores.}\label{tab:archresults}
\end{table}


Transformer-based models consistently outperformed Seq2Seq models, highlighting the effectiveness of self-attention in capturing long-range dependencies. 
The combination of \textbf{Transformer + mBERT} achieved the highest BLEU score, demonstrating the importance of multilingual contextual embeddings for cross-lingual transfer. Models trained with learned embeddings showed limited performance, indicating insufficient semantic alignment across languages.

\subsection{Effect of Dataset Size and Training Epochs}
\begin{table}[]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Epochs}} & \multicolumn{1}{c|}{\textbf{Number of Samples}} & \multicolumn{1}{c|}{\textbf{BLEU Score}} \\ \hline
$10$                                    & $500000$                                         & $1.26$                                     \\ \hline
$10$                                    & $1000000$                                       & $1.30$                                     \\ \hline
$3 $                                    & $4200000$                                       & $9.52$                                     \\ \hline
$5 $                                    & $4200000$                                       & $11.20$                                    \\ \hline
$10$                                    & $4200000$                                       & $13.70$                                    \\ \hline
\end{tabular}%
}
\caption{Dataset scale results on Transformer + mBERT}\label{tab:dataresults}
\end{table}
We further analyzed the impact of dataset scale and number of epochs using the Transformer + mBERT model, the results of which can be viewed in Table~\ref{tab:dataresults}

Increasing the number of training samples resulted in significant improvements in BLEU score.
Even with fewer epochs, large datasets led to better performance than small datasets trained for more epochs.
Additional epochs provided consistent gains when sufficient data was available.
These findings emphasize that \textbf{data scale plays a critical role} in effective cross-lingual transfer.

\subsection{Comparison with Pre-trained Translation Models}
To establish a baseline, we evaluated several pre-trained translation models without fine-tuning. These models are compared in Table~\ref{tab:pretraincomp}

\begin{table}[]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Model}} & \multicolumn{1}{c|}{\textbf{BLEU Score}} \\ \hline
facebook/nllb-200-distilled-600M     & \textbf{$28.40$}                           \\ \hline
Helsinki-NLP/opus-mt-mul-en          & $16.60$                                 \\ \hline
RLM-hinglish-translator              & $15.69$                                   \\ \hline
Helsinki-NLP/opus-mt-hi-en           & $5.34$                                     \\ \hline
facebook/m2m100\_418M                & $1.21$                                     \\ \hline
\end{tabular}%
}
\caption{Pre-Trained Translation Model Scores}\label{tab:pretraincomp}
\end{table}

Large-scale multilingual models such as NLLB-200 significantly outperformed custom-trained models.
Models trained on multilingual or code-mixed data showed better zero-shot generalization.
The performance gap highlights the importance of extensive multilingual pretraining.

\section{Implementation}
We implemented the following models: transformer and seq2seq with mBERT embeddings and compared our performance to baseline scores of learned embeddings.

First we loaded up the embeddings from mBERT
\includegraphics[width=\columnwidth]{tokenizer.png}

Then we tokenized both the source (hindi-english code mixed) and target (english) sentences using our loaded tokenizer. mBERT applies BPE to handle OOV words by breaking them up into subwords. During training, we also shift the target sequence that creates input output pairs for the decoder. Thus, at each timestep, the decoder receives all previous tokens and learns to predict the next token.
\includegraphics[width=\columnwidth]{batching.png}

This is our main training loop using AdamW as the optimizer and Cross Entropy loss. After gradient computation using backpropagation, we also applied a clipping of $1.0$ to prevent explosion of the gradients.
\includegraphics[width=\columnwidth]{trainloop.png}

Then, we calculated the BLEU score using 500 of our test samples. 
\includegraphics[width=\columnwidth]{bleu.png}

\section{Conclusion}
We investigated cross-lingual code-mixed Hindi-English translation in this project experimenting with different embedding strategies and architectures. Our results show that Transformer-based architecture performs better compared to Seq2Seq-based architecture. The main reason behind this would be the self-attention mechanism of Transformers, which supports the capturing long-range dependencies.
 
Using multilingual pre-trained embeddings (i.e., mBERT) produced significantly better results than using learned embeddings and which shows us the importance of cross-lingual semantic alignment between the two languages. Our experiments also demonstrated the importance of large datasets on performance in low-resource environments, with dataset size having a larger impact than the number of training epochs.
 
Our final model achieved a BLEU score of $13.70$, however larger pre-trained models such as NLLB-200 outperforms our model.These findings indicate that models with extensive pretraining outperform all others, even under zero-shot evaluation. Our future research will focus on developing efficient means of fine-tuning large pre-trained models, zero-shot testing on unseen code-mixed data, and cross-lingual transfer to other language pairs.
 
The results support the conclusion that Transformers, combined with multilingual pre-training, form the foundation for building strong models for cross-lingual code-mixed translation.

% Custom bibliography entries only
\section*{Bibliography}
\begin{enumerate}[label = {[\arabic*]}]
    \item K. Kartik, S. Soni, A. Kunchukuttan, T. Chakraborty, and M. S. Akhtar, "Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation," arXiv.org, 2024. \url{https://arxiv.org/abs/2403.16771}.
    \item A. Zhang, Z. Lipton, M. Li, and A. Smola, "The Bahdanau Attention Mechanism - Dive into Deep Learning 1.0.3," D2l.ai, 2014. \url{https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html}
    \item J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," ArXiv, Oct. 11, 2018. \url{https://arxiv.org/abs/1810.04805}
    \item Google, "google-bert/bert-base-multilingual-cased · Hugging Face," huggingface.co, Mar. 11, 2024. \url{https://huggingface.co/google-bert/bert-base-multilingual-cased}
    \item I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to Sequence Learning with Neural Networks," arXiv.org, 2014. \url{https://arxiv.org/abs/1409.3215}
\end{enumerate}

\end{document}
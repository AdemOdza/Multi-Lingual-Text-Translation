{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTGpovUU1U0Y",
        "outputId": "a39cffeb-9d89-4a60-a57d-359dd9a0ca61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GhI4pkE1U0b",
        "outputId": "7128bc41-ed6b-4f6d-c9f2-07ff93e4868e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/hicm.zip\n",
            "  inflating: /content/data/hicm/hicm_corpus.test.txt  \n",
            "  inflating: /content/data/hicm/hicm_corpus.train.txt  \n",
            "  inflating: /content/data/hicm/hicm_corpus.valid.txt  \n",
            "  inflating: /content/data/hicm/hicm_unigram_32000.model  \n",
            "  inflating: /content/data/hicm/hicm_unigram_32000.vocab  \n",
            "  inflating: /content/data/hicm/test.txt  \n",
            "  inflating: /content/data/hicm/train.txt  \n",
            "  inflating: /content/data/hicm/valid.txt  \n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/data\n",
        "\n",
        "!unzip -o \"/content/drive/MyDrive/hicm.zip\" -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWpDY96j1U0b",
        "outputId": "cc434d00-daaa-4d2e-dcb2-ae5ef9cea111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Hindi vocab size: 391146\n",
            "English vocab size: 206248\n",
            "Loaded 4,000,000 samples\n",
            "Loaded 2,507 samples\n",
            "Total parameters: 208,175,785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seq2Seq Epoch 1: 100%|██████████| 62500/62500 [2:51:00<00:00,  6.09it/s, loss=3.8249]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Avg Loss: 5.0503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Seq2Seq Epoch 2: 100%|██████████| 62500/62500 [2:51:52<00:00,  6.06it/s, loss=3.8258]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Avg Loss: 3.8537\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Calculating BLEU on 500 test samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:10<00:00, 45.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Seq2Seq BLEU: 1.83\n",
            "============================================================\n",
            "\n",
            "Sample Translations:\n",
            "\n",
            "1. Source: आपकी Car में black box?\n",
            "   Predicted: In the the Black Selector\n",
            "   Reference: A black box in your car?\n",
            "\n",
            "2. Source: जबकि America के road planner, ध्वस्त होते हुए highway system को सुधारने के लिए money की कमी से जूझ रहे हैं, वहीं बहुत-से people इसका solution छोटे से black box में देख रहे हैं, जो आपकी car के dashboard पर सफ़ाई से fit हो जाता है।\n",
            "   Predicted: While the road of the the the the the to the the the the system to the the the the the the the the the black black lines in the black of the the the the the the the\n",
            "   Reference: As America's road planners struggle to find the cash to mend a crumbling highway system, many are beginning to see a solution in a little black box that fits neatly by the dashboard of your car.\n",
            "\n",
            "3. Source: यह device, जो motor driver द्वारा vehicle चलाए गए प्रत्येक mile को track करती है तथा उस information को officers को transmit करती है, आजकल America की प्रमुख roads का वित्त-पोषण करने के लिए पुराने हो चुके system का जीर्णोद्धार करने के लिए Washington और state planning office के लिए एक controversial effort का issue बन चुका है।\n",
            "   Predicted: This vein which connects each train by a motor driver by the motor driver and the to to the the to the the to the the to the the of the the main steps of the\n",
            "   Reference: The devices, which track every mile a motorist drives and transmit that information to bureaucrats, are at the center of a controversial attempt in Washington and state planning offices to overhaul the outdated system for funding America's major roads.\n",
            "\n",
            "4. Source: आम तौर पर highway planning जैसा boring work भी suddenly intensive debate तथा lively alliances का issue बन गया है।\n",
            "   Predicted: Normally the the planning work is as a as a of of the and and and and\n",
            "   Reference: The usually dull arena of highway planning has suddenly spawned intense debate and colorful alliances.\n",
            "\n",
            "5. Source: आपने द्वारा drive किए गए mile, तथा possibly drive किए गए location का description रखने - और फिर इस information का use tax bill तैयार करने के लिए - government को इन black box का use करने की permission देने के side में support जुटाने के लिए Libertarian environmental दलs के साथ मिल गए हैं।\n",
            "   Predicted: The instructions and the given to the the and the the and the drive and and and and the use of the information to the the Government to use the Government to use the use of the black\n",
            "   Reference: Libertarians have joined environmental groups in lobbying to allow government to use the little boxes to keep track of the miles you drive, and possibly where you drive them - then use the information to draw up a tax bill.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1. Setup\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# 2. File Paths\n",
        "# ============================================================\n",
        "BASE = Path(\"/content/data/hicm\")\n",
        "\n",
        "train_inp = BASE / \"hicm_corpus.train.txt\"\n",
        "test_inp  = BASE / \"hicm_corpus.test.txt\"\n",
        "valid_inp = BASE / \"hicm_corpus.valid.txt\"\n",
        "\n",
        "train_out = BASE / \"train.txt\"\n",
        "test_out  = BASE / \"test.txt\"\n",
        "valid_out = BASE / \"valid.txt\"\n",
        "\n",
        "# ============================================================\n",
        "# 3. Tokenization and vocab\n",
        "# ============================================================\n",
        "def tokenize(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "def build_vocab(file, add_special_tokens=True, limit=None):\n",
        "    vocab = {}\n",
        "    idx = 0\n",
        "    if add_special_tokens:\n",
        "        vocab = {\"<PAD>\":0, \"<UNK>\":1, \"<BOS>\":2, \"<EOS>\":3}\n",
        "        idx = 4\n",
        "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if limit and i >= limit:\n",
        "                break\n",
        "            for tok in tokenize(line):\n",
        "                if tok not in vocab:\n",
        "                    vocab[tok] = idx\n",
        "                    idx += 1\n",
        "    return vocab\n",
        "\n",
        "# TRAIN_LIMIT = 4000000 ####change this to limit number of training samples\n",
        "\n",
        "src_vocab = build_vocab(train_inp, limit=TRAIN_LIMIT)\n",
        "tgt_vocab = build_vocab(train_out, limit=TRAIN_LIMIT)\n",
        "inv_tgt_vocab = {v:k for k,v in tgt_vocab.items()}\n",
        "\n",
        "SRC_VOCAB_SIZE = len(src_vocab)\n",
        "TGT_VOCAB_SIZE = len(tgt_vocab)\n",
        "print(\"Hindi vocab size:\", SRC_VOCAB_SIZE)\n",
        "print(\"English vocab size:\", TGT_VOCAB_SIZE)\n",
        "\n",
        "# ============================================================\n",
        "# 4. Encoding functions\n",
        "# ============================================================\n",
        "MAX_LEN = 40\n",
        "\n",
        "def encode_src(sentence):\n",
        "    toks = tokenize(sentence)[:MAX_LEN]\n",
        "    ids = [src_vocab.get(t, src_vocab[\"<UNK>\"]) for t in toks]\n",
        "    return ids\n",
        "\n",
        "def encode_tgt(sentence):\n",
        "    toks = tokenize(sentence)[:MAX_LEN]\n",
        "    ids = [tgt_vocab[\"<BOS>\"]] + [tgt_vocab.get(t, tgt_vocab[\"<UNK>\"]) for t in toks] + [tgt_vocab[\"<EOS>\"]]\n",
        "    return ids\n",
        "\n",
        "# ============================================================\n",
        "# 5. Dataset and collate\n",
        "# ============================================================\n",
        "class HinEngDataset(Dataset):\n",
        "    def __init__(self, src_file, tgt_file, limit=None):\n",
        "        self.src_lines = open(src_file, \"r\", encoding=\"utf-8\").read().splitlines()\n",
        "        self.tgt_lines = open(tgt_file, \"r\", encoding=\"utf-8\").read().splitlines()\n",
        "        if limit:\n",
        "            self.src_lines = self.src_lines[:limit]\n",
        "            self.tgt_lines = self.tgt_lines[:limit]\n",
        "        assert len(self.src_lines) == len(self.tgt_lines)\n",
        "        print(f\"Loaded {len(self.src_lines):,} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return encode_src(self.src_lines[idx]), encode_tgt(self.tgt_lines[idx])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    max_src = max(len(x) for x in src_batch)\n",
        "    max_tgt = max(len(x) for x in tgt_batch)\n",
        "\n",
        "    src_tensor = torch.full((len(batch), max_src), src_vocab[\"<PAD>\"], dtype=torch.long)\n",
        "    src_mask   = torch.zeros(len(batch), max_src, dtype=torch.bool)\n",
        "    tgt_tensor = torch.full((len(batch), max_tgt), tgt_vocab[\"<PAD>\"], dtype=torch.long)\n",
        "\n",
        "    for i, seq in enumerate(src_batch):\n",
        "        src_tensor[i, :len(seq)] = torch.tensor(seq)\n",
        "        src_mask[i, :len(seq)] = 1\n",
        "    for i, seq in enumerate(tgt_batch):\n",
        "        tgt_tensor[i, :len(seq)] = torch.tensor(seq)\n",
        "\n",
        "    return src_tensor.to(device), src_mask.to(device), tgt_tensor.to(device)\n",
        "\n",
        "train_ds = HinEngDataset(train_inp, train_out, limit=TRAIN_LIMIT)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "test_ds = HinEngDataset(test_inp, test_out)\n",
        "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ============================================================\n",
        "# 6. Seq2Seq Model with Bahdanau Attention\n",
        "# ============================================================\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.W_decoder = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.W_encoder = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs, src_mask):\n",
        "        # decoder_hidden: [B, hidden_dim]\n",
        "        # encoder_outputs: [B, src_len, hidden_dim]\n",
        "        # src_mask: [B, src_len] (True = valid, False = padding)\n",
        "        batch_size, src_len, hidden_dim = encoder_outputs.shape\n",
        "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)  # [B, src_len, hidden_dim]\n",
        "\n",
        "        energy = torch.tanh(self.W_decoder(decoder_hidden) + self.W_encoder(encoder_outputs))\n",
        "        scores = self.v(energy).squeeze(-1)  # [B, src_len]\n",
        "        scores = scores.masked_fill(~src_mask, -1e10)  # mask padding\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=1)  # [B, src_len]\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # [B, hidden_dim]\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim // 2, num_layers,\n",
        "                           bidirectional=True, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # src: [B, src_len]\n",
        "        embedded = self.dropout(self.embed(src))  # [B, src_len, emb_dim]\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        # outputs: [B, src_len, hidden_dim]\n",
        "        # hidden, cell: [num_layers*2, B, hidden_dim//2]\n",
        "\n",
        "        # Combine bidirectional hidden states\n",
        "        hidden = self._bridge(hidden)  # [num_layers, B, hidden_dim]\n",
        "        cell = self._bridge(cell)\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "    def _bridge(self, states):\n",
        "        # states: [num_layers*2, B, hidden_dim//2]\n",
        "        num_layers = states.size(0) // 2\n",
        "        states = states.view(num_layers, 2, states.size(1), states.size(2))\n",
        "        return torch.cat([states[:, 0], states[:, 1]], dim=-1)  # [num_layers, B, hidden_dim]\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.attention = BahdanauAttention(hidden_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim + hidden_dim, hidden_dim, num_layers,\n",
        "                           dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs, src_mask):\n",
        "        # input_token: [B]\n",
        "        # hidden, cell: [num_layers, B, hidden_dim]\n",
        "        # encoder_outputs: [B, src_len, hidden_dim]\n",
        "\n",
        "        embedded = self.dropout(self.embed(input_token)).unsqueeze(1)  # [B, 1, emb_dim]\n",
        "\n",
        "        # Compute attention using last layer's hidden state\n",
        "        context, _ = self.attention(hidden[-1], encoder_outputs, src_mask)  # [B, hidden_dim]\n",
        "\n",
        "        # Concatenate embedded input and context\n",
        "        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)  # [B, 1, emb_dim + hidden_dim]\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        # output: [B, 1, hidden_dim]\n",
        "\n",
        "        prediction = self.out(output.squeeze(1))  # [B, vocab_size]\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, emb_dim=256, hidden_dim=256, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, emb_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab, emb_dim, hidden_dim, num_layers, dropout)\n",
        "        self.tgt_vocab_size = tgt_vocab\n",
        "\n",
        "    def forward(self, src, src_mask, tgt, teacher_forcing_ratio=0.5):\n",
        "        # src: [B, src_len]\n",
        "        # tgt: [B, tgt_len]\n",
        "        batch_size = src.size(0)\n",
        "        tgt_len = tgt.size(1)\n",
        "\n",
        "        # Encode\n",
        "        encoder_outputs, hidden, cell = self.encoder(src, src_mask)\n",
        "\n",
        "        # Prepare for decoding\n",
        "        outputs = []\n",
        "        input_token = tgt[:, 0]  # <BOS> token\n",
        "\n",
        "        # Decode step by step\n",
        "        for t in range(1, tgt_len):\n",
        "            prediction, hidden, cell = self.decoder(input_token, hidden, cell, encoder_outputs, src_mask)\n",
        "            outputs.append(prediction)\n",
        "\n",
        "            # Teacher forcing: use ground truth or prediction\n",
        "            use_teacher = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            input_token = tgt[:, t] if use_teacher else prediction.argmax(dim=1)\n",
        "\n",
        "        return torch.stack(outputs, dim=1)  # [B, tgt_len-1, vocab_size]\n",
        "\n",
        "model = Seq2Seq(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<PAD>\"])\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# ============================================================\n",
        "# 7. Training Loop\n",
        "# ============================================================\n",
        "def train_epoch(epoch_num):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Seq2Seq Epoch {epoch_num}\")\n",
        "    for src, src_mask, tgt in pbar:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, src_mask, tgt)\n",
        "        # logits: [B, tgt_len-1, vocab_size]\n",
        "        # Compare with tgt[:,1:] (skip <BOS>)\n",
        "        loss = criterion(logits.reshape(-1, TGT_VOCAB_SIZE), tgt[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "    return total / len(train_loader)\n",
        "\n",
        "for epoch in range(2):\n",
        "    loss = train_epoch(epoch+1)\n",
        "    print(f\"Epoch {epoch+1} | Avg Loss: {loss:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 8. Inference (Greedy Decoding)\n",
        "# ============================================================\n",
        "def translate(sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_ids = torch.tensor([encode_src(sentence)], dtype=torch.long).to(device)\n",
        "        mask = torch.ones(1, src_ids.size(1), dtype=torch.bool).to(device)\n",
        "\n",
        "        # Encode\n",
        "        encoder_outputs, hidden, cell = model.encoder(src_ids, mask)\n",
        "\n",
        "        # Decode\n",
        "        input_token = torch.tensor([tgt_vocab[\"<BOS>\"]], dtype=torch.long).to(device)\n",
        "        translated = []\n",
        "\n",
        "        for _ in range(MAX_LEN):\n",
        "            prediction, hidden, cell = model.decoder(input_token, hidden, cell, encoder_outputs, mask)\n",
        "            next_tok = prediction.argmax(dim=1)\n",
        "\n",
        "            if next_tok.item() == tgt_vocab[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "            translated.append(inv_tgt_vocab.get(next_tok.item(), \"<UNK>\"))\n",
        "            input_token = next_tok\n",
        "\n",
        "    return \" \".join(translated)\n",
        "\n",
        "# ============================================================\n",
        "# 9. BLEU Evaluation\n",
        "# ============================================================\n",
        "!pip install -q sacrebleu\n",
        "\n",
        "from sacrebleu.metrics import BLEU\n",
        "\n",
        "def calculate_bleu(num_samples=500):\n",
        "    test_src_lines = open(test_inp, \"r\", encoding=\"utf-8\").read().splitlines()[:num_samples]\n",
        "    test_tgt_lines = open(test_out, \"r\", encoding=\"utf-8\").read().splitlines()[:num_samples]\n",
        "\n",
        "    print(f\"\\nCalculating BLEU on {len(test_src_lines)} test samples...\")\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for src_text, ref_text in tqdm(zip(test_src_lines, test_tgt_lines), total=len(test_src_lines)):\n",
        "        pred_text = translate(src_text)\n",
        "        predictions.append(pred_text)\n",
        "        references.append(ref_text)\n",
        "\n",
        "    bleu = BLEU()\n",
        "    score = bleu.corpus_score(predictions, [references])\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Seq2Seq BLEU: {score.score:.2f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(\"\\nSample Translations:\")\n",
        "    for i in range(5):\n",
        "        print(f\"\\n{i+1}. Source: {test_src_lines[i]}\")\n",
        "        print(f\"   Predicted: {predictions[i]}\")\n",
        "        print(f\"   Reference: {references[i]}\")\n",
        "\n",
        "    return score.score\n",
        "\n",
        "bleu_score = calculate_bleu(500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5p9kLF3h2QHa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}